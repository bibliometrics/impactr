---
title: "Impact from Traditional Metrics"
date: "`r Sys.Date()`"
always_allow_html: yes
output:
  md_document:
    variant: gfm
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = FALSE)
library(tidyverse);library(magrittr)

pmid <- readRDS("~/starsurg_impact/data/impact_pub.rds")$pmid

data <- impactr::extract_pmid(pmid) %>%
    # ensure the input journal name is correct (see extract_journal_metric()$unmatch)
    dplyr::mutate(journal_edit = ifelse(is.na(journal_full2)==T, journal_full,journal_full2))  %>%
    dplyr::mutate(journal_edit = ifelse(journal_edit=="The Lancet", "Lancet", journal_edit),
                  journal_edit = ifelse(journal_edit=="British Medical Journal", "BMJ-British Medical Journal", journal_edit))
```

# Research Impact

Citations are a common metric for determining the impact of research on the wider academic field. Both `extract_pmid()` and `extract_doi()` will provide data on citations (based on PubMed/Crossref repositories) and journal impact factor (based on [Scimago Scientific Journal Rankings](https://www.scimagojr.com)).

## **`impact_cite()`**

The function `impact_cite()` can also produce several metrics that can be used for the purposes of assessment of the more traditional research impact of publications (using the [Scholar](https://cran.r-project.org/web/packages/scholar/index.html) package). This includes:

- Journal-Level: Impact Factor (IF) and Eigenfactor.

- Paper-Level: Citations (including H-Index) in both Google Scholar (`cite_gs`) and CrossRef / Pubmed repositories (`cite_cr` or `cite_pm`)

```{r cite_data, warning=FALSE, message=FALSE}
data_cite <- impactr::impact_cite(df = data, scholar_id = "Ol5uNSwAAAAJ&hl", var_citation = "cite_gs")
                             
```
```{r cite_data_out, warning=FALSE, message=FALSE, echo=FALSE}
data_cite$output %>%
  dplyr::select(doi, title, journal_edit, journal_if,
                cite_pm,	cite_gs) %>%
  knitr::kable(format="html") %>%
  kableExtra::column_spec(which(colnames(.) %in% c("title")), width_min="7in") %>%
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = F) %>%
  kableExtra::scroll_box(height = "400px",width = "1000px")
```

&nbsp;

## **Impact Metrics**
### **Citations**
Citations (e.g. `cite_total`) can be used to either provide summary statistics or visualisations.

**Note**: Google Scholar is typically being more sensitive but less specific in estimating citation count than Crossref / PubMed repositories.

```{r cite_plot1, warning=FALSE, message=FALSE, echo=FALSE, fig.height = 5.76*0.75, fig.width = 9.60*0.75, fig.align = "center"}
cite_max <- data_cite$output %$% max(cite_gs, na.rm=TRUE)

data_cite$output %>%
  dplyr::select(cite_gs, cite_pm) %>%
  ggplot() +
  geom_point(aes(x = cite_pm, y = cite_gs)) + 
  scale_x_continuous(breaks=c(seq(0, cite_max, by=5)),
                     limits = c(0, max(data_cite$output$cite_gs, na.rm=TRUE)), expand = c(0, 0.5)) +
  scale_y_continuous(breaks=c(seq(0, cite_max, by=5)),
                     limits = c(0, max(data_cite$output$cite_gs, na.rm=TRUE)), expand = c(0, 0.5)) + 
  labs(x = "Citations per publication on PubMed", y = "Citations per publication on Google Scholar") +
  theme_bw()
```

&nbsp;

### **Journals**
As journal-level information is also extracted using `impact_cite()` (e.g. impact factor / Eigenfactor) this can also allow some assessment of impact of the articles in relation to the journal benchmark.

For example, the plot below demonstrates the ratio of Paper Citations : Journal Impact Factor - any point above the horizontal line (ratio of 1:1) indicates the paper has gathered more citations than typical for that journal.

```{r journal_plot, warning=FALSE, message=FALSE, echo=FALSE,fig.height = 5.76*0.75, fig.width = 9.60*0.75, fig.align = "center"}
cite_max <- data_cite$output %$% max(cite_gs/journal_if, na.rm=TRUE)

data_cite$output %>%
  dplyr::filter(is.na(journal_if)==F) %>%
  dplyr::mutate(cite_year = cite_gs/(lubridate::year(Sys.Date())-year)) %>%
  dplyr::mutate(cite_if = cite_gs / journal_if,
                cite_if_year = cite_year / journal_if) %>%
  dplyr::filter(type=="Paper") %>%
  dplyr::select("Journal" = journal_full2, journal_if,
                cite_gs, cite_year, cite_if, cite_if_year, year, almetric) %>%
  dplyr::group_by(Journal, journal_if, cite_gs,
                  cite_year, cite_if, cite_if_year, year, almetric) %>%
  dplyr::summarise(n = n()) %>%
  ggplot() + 
  aes(x = year, y = cite_if, group = Journal, colour = Journal, fill = Journal) +
  geom_point() +
  scale_y_continuous(breaks=c(0, 1, seq(5, cite_max, by=5)),minor_breaks=NULL) +
  geom_hline(aes(yintercept=1), colour = "dark grey", linetype = "dashed") +
  labs(x = "Year of Publication", y = "Ratio of Paper Citations : Journal Impact Factor") +
  theme_bw()

```

&nbsp;

### **Metrics**
Citation metrics are author-level measures that attempts to assess both the productivity and citation impact of the publications. These include:

 - **Total citations**: Total number of citations that they have received in other publications.
 
 - **[H-Index](https://harzing.com/popbook/ch1_4_2.htm)**: Based on the set of the scientist's most cited papers and the number of citations that they have received in other publications.
 
 - **[M-Quotient](https://harzing.com/popbook/ch1_4_3.htm)**: A method to facilitate comparisons between academics with different lengths of academic careers. This divides the h-index by the number of years the academic has been active (measured as the number of years since the first published paper)
 
 - **[G-Index](https://harzing.com/popbook/ch1_4_6.htm)**: It aims to improve on the h-index by giving more weight to highly-cited articles. The H-Index can "undervalue" highly cited papers as it ignores the number of citations to each individual article beyond what is needed to achieve a certain h-index.

`impact_cite()` will produce common citation metrics automatically based on the papers included in the dataframe supplied. Alternatively this can be calculated directly using `impact_cite_metric()`. 

```{r cite_hindex, warning=FALSE, message=FALSE}
data_cite$metric; impactr::impact_cite_metric(data_cite$output$cite_gs, data_cite$output$year)
```

&nbsp;

## **Troubleshooting (`$validation`) **

The [Google Scholar ID](https://en.wikipedia.org/wiki/Template:Google_Scholar_id) is used to derive publications, and so all publications by an author (or authorship group) must be uploaded under one Google Scholar account.

As google scholar does not contain DOI or PMID, papers must be matched by title, and can only be matched if there is a google scholar record for each paper in the supplied dataframe. `impact_cite()` provides several features to proactively identify issues via `$validation` output. 

The outcome will record either `matched`, or the following:
 
* **`noscholar`**: If there is no corresponding google scholar record for the publication, then these will be listed here.

* **`unmatch`**: If the google scholar records were unable to be matched to the existing dataset (by title), then these will be listed here.
